{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da2299ce-6063-43fb-b54b-d0f19f2169d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1539786805.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    conda install pandas openpyxl\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "conda install pandas openpyxl\n",
    "pip install gspread google-auth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6049bd9b-e83f-496b-8d4f-58a573e92351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Supertails\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - openpyxl\n",
      "    - pandas\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    openpyxl-3.1.5             |  py312h827c3e9_1         649 KB\n",
      "    pandas-2.2.3               |  py312h5da7b33_0        14.2 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        14.9 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  openpyxl                            3.1.5-py312h827c3e9_0 --> 3.1.5-py312h827c3e9_1 \n",
      "  pandas                              2.2.2-py312h0158946_0 --> 2.2.3-py312h5da7b33_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working...\n",
      "pandas-2.2.3         | 14.2 MB   |            |   0% \n",
      "\n",
      "openpyxl-3.1.5       | 649 KB    |            |   0% \u001b[A\n",
      "pandas-2.2.3         | 14.2 MB   |            |   1% \n",
      "\n",
      "openpyxl-3.1.5       | 649 KB    | #9         |  20% \u001b[A\n",
      "\n",
      "openpyxl-3.1.5       | 649 KB    | ########## | 100% \u001b[A\n",
      "pandas-2.2.3         | 14.2 MB   | 6          |   6% \n",
      "pandas-2.2.3         | 14.2 MB   | #3         |  13% \n",
      "pandas-2.2.3         | 14.2 MB   | ##1        |  21% \n",
      "pandas-2.2.3         | 14.2 MB   | ##9        |  29% \n",
      "pandas-2.2.3         | 14.2 MB   | ###6       |  37% \n",
      "pandas-2.2.3         | 14.2 MB   | ####5      |  45% \n",
      "pandas-2.2.3         | 14.2 MB   | #####7     |  57% \n",
      "\n",
      "openpyxl-3.1.5       | 649 KB    | ########## | 100% \u001b[A\n",
      "\n",
      "openpyxl-3.1.5       | 649 KB    | ########## | 100% \u001b[A\n",
      "pandas-2.2.3         | 14.2 MB   | #######    |  71% \n",
      "pandas-2.2.3         | 14.2 MB   | ########5  |  86% \n",
      "pandas-2.2.3         | 14.2 MB   | #########9 | 100% \n",
      "pandas-2.2.3         | 14.2 MB   | ########## | 100% \n",
      "pandas-2.2.3         | 14.2 MB   | ########## | 100% \n",
      "                                                     \n",
      "\n",
      "\n",
      "                                                     \u001b[A done\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Requirement already satisfied: gspread in c:\\users\\supertails\\anaconda3\\lib\\site-packages (6.2.0)\n",
      "Requirement already satisfied: google-auth in c:\\users\\supertails\\anaconda3\\lib\\site-packages (2.39.0)\n",
      "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in c:\\users\\supertails\\anaconda3\\lib\\site-packages (from gspread) (1.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\supertails\\anaconda3\\lib\\site-packages (from google-auth) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\supertails\\anaconda3\\lib\\site-packages (from google-auth) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\supertails\\anaconda3\\lib\\site-packages (from google-auth) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\supertails\\anaconda3\\lib\\site-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\supertails\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\supertails\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\supertails\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\supertails\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\supertails\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\supertails\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\supertails\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!conda install pandas openpyxl -y\n",
    "!pip install gspread google-auth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1300528-6918-4e9e-b65f-bd1109cfaa6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'service_account_key.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m SCOPES \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.googleapis.com/auth/spreadsheets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.googleapis.com/auth/drive\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m SERVICE_ACCOUNT_FILE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mservice_account_key.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 9\u001b[0m credentials \u001b[38;5;241m=\u001b[39m Credentials\u001b[38;5;241m.\u001b[39mfrom_service_account_file(\n\u001b[0;32m     10\u001b[0m     SERVICE_ACCOUNT_FILE, scopes\u001b[38;5;241m=\u001b[39mSCOPES)\n\u001b[0;32m     11\u001b[0m client \u001b[38;5;241m=\u001b[39m gspread\u001b[38;5;241m.\u001b[39mauthorize(credentials)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Step 2: Load Excel files\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\oauth2\\service_account.py:260\u001b[0m, in \u001b[0;36mCredentials.from_service_account_file\u001b[1;34m(cls, filename, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_service_account_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a Credentials instance from a service account json file.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m            credentials.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m     info, signer \u001b[38;5;241m=\u001b[39m _service_account_info\u001b[38;5;241m.\u001b[39mfrom_filename(\n\u001b[0;32m    261\u001b[0m         filename, require\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient_email\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_uri\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    262\u001b[0m     )\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_signer_and_info(signer, info, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\auth\\_service_account_info.py:78\u001b[0m, in \u001b[0;36mfrom_filename\u001b[1;34m(filename, require, use_rsa_signer)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_filename\u001b[39m(filename, require\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, use_rsa_signer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads a Google service account JSON file and returns its parsed info.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m            info and a signer instance.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[0;32m     79\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data, from_dict(data, require\u001b[38;5;241m=\u001b[39mrequire, use_rsa_signer\u001b[38;5;241m=\u001b[39muse_rsa_signer)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'service_account_key.json'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel('Export.xlsx')\n",
    "base_nf_df = pd.read_excel('Base_NF.xlsx')\n",
    "\n",
    "# Step 3: Merge MRP into base sales file\n",
    "merged_df = base_nf_df.merge(export_df[['SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='SKU', right_on='SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbulter', 'Truelove']\n",
    "vendor_df = merged_df[merged_df['Vendor'].isin(vendors)]\n",
    "\n",
    "# Step 6: Create weekly report (last 7 days)\n",
    "vendor_df['Order Date'] = pd.to_datetime(vendor_df['Order Date'])  # adjust column name if needed\n",
    "last_7_days = vendor_df[vendor_df['Order Date'] >= pd.Timestamp.now() - pd.Timedelta(days=7)]\n",
    "\n",
    "# Step 7: Group data for Dashboard\n",
    "dashboard_df = last_7_days.groupby(['Product title at time of sale', 'Product type']).agg({\n",
    "    'Offtake': 'sum',\n",
    "    'Net items sold': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Net items sold': 'Daily Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 8: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Vendor_Weekly_Dashboard').worksheet('Dashboard')  # change if needed\n",
    "dashboard_sheet.clear()\n",
    "\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(\"Dashboard updated successfully 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63385994-13e2-4268-96ab-05ecd32b0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT_FILE = 'C:/Users/YourName/Downloads/service_account_key.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97ce9d9b-c5fd-4479-9930-012b7204c108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Supertails'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58691470-18fe-4dfd-82b7-88f865b1eca6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2217828659.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    SERVICE_A\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_A\n",
    "CCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel('Export.xlsx')\n",
    "base_nf_df = pd.read_excel('Base_NF.xlsx')\n",
    "\n",
    "# Step 3: Merge MRP into base sales file\n",
    "merged_df = base_nf_df.merge(export_df[['SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='SKU', right_on='SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbulter', 'Truelove']\n",
    "vendor_df = merged_df[merged_df['Vendor'].isin(vendors)]\n",
    "\n",
    "# Step 6: Create weekly report (last 7 days)\n",
    "vendor_df['Order Date'] = pd.to_datetime(vendor_df['Order Date'])  # adjust column name if needed\n",
    "last_7_days = vendor_df[vendor_df['Order Date'] >= pd.Timestamp.now() - pd.Timedelta(days=7)]\n",
    "\n",
    "# Step 7: Group data for Dashboard\n",
    "dashboard_df = last_7_days.groupby(['Product title at time of sale', 'Product type']).agg({\n",
    "    'Offtake': 'sum',\n",
    "    'Net items sold': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Net items sold': 'Daily Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 8: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Vendor_Weekly_Dashboard').worksheet('Dashboard')  # change if needed\n",
    "dashboard_sheet.clear()\n",
    "\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(\"Dashboard updated successfully 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ae31b54-efb0-4a92-bb1e-8f6a9ad6bd70",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'service_account_key.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m SCOPES \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.googleapis.com/auth/spreadsheets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.googleapis.com/auth/drive\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m SERVICE_ACCOUNT_FILE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mservice_account_key.json\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Make sure this file is in same folder\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m credentials \u001b[38;5;241m=\u001b[39m Credentials\u001b[38;5;241m.\u001b[39mfrom_service_account_file(\n\u001b[0;32m     10\u001b[0m     SERVICE_ACCOUNT_FILE, scopes\u001b[38;5;241m=\u001b[39mSCOPES)\n\u001b[0;32m     11\u001b[0m client \u001b[38;5;241m=\u001b[39m gspread\u001b[38;5;241m.\u001b[39mauthorize(credentials)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Step 2: Load Excel files\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\oauth2\\service_account.py:260\u001b[0m, in \u001b[0;36mCredentials.from_service_account_file\u001b[1;34m(cls, filename, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_service_account_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a Credentials instance from a service account json file.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m            credentials.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m     info, signer \u001b[38;5;241m=\u001b[39m _service_account_info\u001b[38;5;241m.\u001b[39mfrom_filename(\n\u001b[0;32m    261\u001b[0m         filename, require\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient_email\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_uri\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    262\u001b[0m     )\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_signer_and_info(signer, info, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\auth\\_service_account_info.py:78\u001b[0m, in \u001b[0;36mfrom_filename\u001b[1;34m(filename, require, use_rsa_signer)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_filename\u001b[39m(filename, require\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, use_rsa_signer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads a Google service account JSON file and returns its parsed info.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m            info and a signer instance.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[0;32m     79\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data, from_dict(data, require\u001b[38;5;241m=\u001b[39mrequire, use_rsa_signer\u001b[38;5;241m=\u001b[39muse_rsa_signer)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'service_account_key.json'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'  # Make sure this file is in same folder\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel('Export.xlsx')  # Your export file\n",
    "base_nf_df = pd.read_excel('Base_NF.xlsx')  # Your base file\n",
    "\n",
    "# Step 3: Merge MRP into base sales file\n",
    "merged_df = base_nf_df.merge(export_df[['SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='SKU', right_on='SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbulter', 'Truelove']\n",
    "vendor_df = merged_df[merged_df['Vendor'].isin(vendors)]\n",
    "\n",
    "# Step 6: Create weekly report (last 7 days)\n",
    "vendor_df['Order Date'] = pd.to_datetime(vendor_df['Order Date'])  # Make sure this column exists\n",
    "last_7_days = vendor_df[vendor_df['Order Date'] >= pd.Timestamp.now() - pd.Timedelta(days=7)]\n",
    "\n",
    "# Step 7: Group data for Dashboard\n",
    "dashboard_df = last_7_days.groupby(['Product title at time of sale', 'Product type']).agg({\n",
    "    'Offtake': 'sum',\n",
    "    'Net items sold': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Net items sold': 'Daily Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 8: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Vendor_Weekly_Dashboard').worksheet('Dashboard')  # Sheet name must exist\n",
    "dashboard_sheet.clear()\n",
    "\n",
    "# Update the sheet\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(\"Dashboard updated successfully 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9525d34c-3f93-4f80-8244-0c0a07d09483",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'service_account_key.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m credentials \u001b[38;5;241m=\u001b[39m Credentials\u001b[38;5;241m.\u001b[39mfrom_service_account_file(\n\u001b[0;32m      2\u001b[0m     SERVICE_ACCOUNT_FILE, scopes\u001b[38;5;241m=\u001b[39mSCOPES)\n\u001b[0;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m gspread\u001b[38;5;241m.\u001b[39mauthorize(credentials)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\oauth2\\service_account.py:260\u001b[0m, in \u001b[0;36mCredentials.from_service_account_file\u001b[1;34m(cls, filename, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_service_account_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a Credentials instance from a service account json file.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m            credentials.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m     info, signer \u001b[38;5;241m=\u001b[39m _service_account_info\u001b[38;5;241m.\u001b[39mfrom_filename(\n\u001b[0;32m    261\u001b[0m         filename, require\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient_email\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_uri\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    262\u001b[0m     )\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_signer_and_info(signer, info, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\auth\\_service_account_info.py:78\u001b[0m, in \u001b[0;36mfrom_filename\u001b[1;34m(filename, require, use_rsa_signer)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_filename\u001b[39m(filename, require\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, use_rsa_signer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads a Google service account JSON file and returns its parsed info.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m            info and a signer instance.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[0;32m     79\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data, from_dict(data, require\u001b[38;5;241m=\u001b[39mrequire, use_rsa_signer\u001b[38;5;241m=\u001b[39muse_rsa_signer)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'service_account_key.json'"
     ]
    }
   ],
   "source": [
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f3b648a-7470-4f3a-8dd9-462a07116a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Supertails\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c4ea82a-60b5-4b1a-9af3-a4a07c742641",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['SKU'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 21\u001b[0m\n\u001b[0;32m     15\u001b[0m base_nf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSupertails\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbase_nf.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Step 3: Merge MRP into base sales file\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m base_nf_df\u001b[38;5;241m.\u001b[39mmerge(export_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSKU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVariant Compare At Price\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[0;32m     22\u001b[0m                              how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSKU\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSKU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Step 4: Create 'Returned' and 'Offtake' columns\u001b[39;00m\n\u001b[0;32m     25\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReturned\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNet items sold\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNO\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['SKU'] not in index\""
     ]
    }
   ],
   "source": [
    "    import pandas as pd\n",
    "    import gspread\n",
    "    from google.oauth2.service_account import Credentials\n",
    "    \n",
    "    # Step 1: Setup credentials\n",
    "    SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "    \n",
    "    credentials = Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    client = gspread.authorize(credentials)\n",
    "    \n",
    "    # Step 2: Load Excel files\n",
    "    export_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\Export_2025-04-21_131843 (5).xlsx\")\n",
    "    base_nf_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\base_nf.xlsx\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Step 3: Merge MRP into base sales file\n",
    "    merged_df = base_nf_df.merge(export_df[['SKU', 'Variant Compare At Price']],\n",
    "                                 how='left', left_on='SKU', right_on='SKU')\n",
    "    \n",
    "    # Step 4: Create 'Returned' and 'Offtake' columns\n",
    "    merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "    merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "    \n",
    "    # Step 5: Filter vendors\n",
    "    vendors = ['Barkbulter', 'Truelove']\n",
    "    vendor_df = merged_df[merged_df['Vendor'].isin(vendors)]\n",
    "    \n",
    "    # Step 6: Create weekly report (last 7 days)\n",
    "    vendor_df['Order Date'] = pd.to_datetime(vendor_df['Order Date'])  # adjust column name if needed\n",
    "    last_7_days = vendor_df[vendor_df['Order Date'] >= pd.Timestamp.now() - pd.Timedelta(days=7)]\n",
    "    \n",
    "    \n",
    "    # Step 7: Group data for Dashboard\n",
    "    dashboard_df = last_7_days.groupby(['Product title at time of sale', 'Product type']).agg({\n",
    "        'Offtake': 'sum',\n",
    "        'Net items sold': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    dashboard_df.rename(columns={\n",
    "        'Net items sold': 'Daily Sale Units',\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Step 8: Upload to Google Sheets\n",
    "    dashboard_sheet = client.open('Pupscribe weekly  sales dashboard').worksheet('Dashboard')  # change if needed\n",
    "    dashboard_sheet.clear()\n",
    "    \n",
    "    dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "    \n",
    "    print(\"Dashboard updated successfully 🚀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85e8db65-bd14-483f-8166-304f5fc9f35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Handle', 'Command', 'Title', 'Body HTML', 'Vendor', 'Type', 'Tags', 'Tags Command', 'Created At', 'Updated At', 'Status', 'Published', 'Published At', 'Published Scope', 'Template Suffix', 'Gift Card', 'URL', 'Total Inventory Qty', 'Row #', 'Top Row', 'Variant Inventory Item ID', 'Variant ID', 'Variant Command', 'Option1 Name', 'Option1 Value', 'Option2 Name', 'Option2 Value', 'Option3 Name', 'Option3 Value', 'Variant Position', 'Variant SKU', 'Variant Barcode', 'Variant Image', 'Variant Weight', 'Variant Weight Unit', 'Variant Price', 'Variant Compare At Price', 'Variant Taxable', 'Variant Tax Code', 'Variant Inventory Tracker', 'Variant Inventory Policy', 'Variant Fulfillment Service', 'Variant Requires Shipping', 'Variant Inventory Qty', 'Variant Inventory Adjust', 'Variant Cost']\n"
     ]
    }
   ],
   "source": [
    "print(export_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d646e805-313c-4e54-a4c6-190b05024e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard updated successfully 🚀\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\Export_2025-04-21_131843 (5).xlsx\")\n",
    "base_nf_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\base_nf.xlsx\")\n",
    "\n",
    "# Step 3: Merge MRP into base sales file using 'Variant SKU' instead of 'SKU'\n",
    "merged_df = base_nf_df.merge(export_df[['Variant SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='Product variant SKU', right_on='Variant SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbulter', 'Truelove']\n",
    "vendor_df = merged_df[merged_df['Product vendor'].isin(vendors)]\n",
    "\n",
    "# Step 6: Create weekly report (last 7 days)\n",
    "vendor_df['Day'] = pd.to_datetime(vendor_df['Day'])  # adjust column name if needed\n",
    "last_7_days = vendor_df[vendor_df['Day'] >= pd.Timestamp.now() - pd.Timedelta(days=7)]\n",
    "\n",
    "# Step 7: Group data for Dashboard\n",
    "dashboard_df = last_7_days.groupby(['Product title at time of sale', 'Product type']).agg({\n",
    "    'Offtake': 'sum',\n",
    "    'Net items sold': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Net items sold': 'Daily Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 8: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Pupscribe weekly  sales dashboard').worksheet('Dashboard')  # change if needed\n",
    "dashboard_sheet.clear()\n",
    "\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(\"Dashboard updated successfully 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8318aeb6-5586-4f05-b686-d41565479027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard\n"
     ]
    }
   ],
   "source": [
    "# List all worksheets in the spreadsheet\n",
    "spreadsheet = client.open('Pupscribe weekly  sales dashboard')\n",
    "worksheets = spreadsheet.worksheets()\n",
    "for worksheet in worksheets:\n",
    "    print(worksheet.title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "681dc9a2-7eb7-4132-9c6f-b79a2bee210b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['variant SKU'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m base_nf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSupertails\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbase_nf.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Step 3: Merge MRP into base sales file using 'Product variant SKU' and 'variant SKU'\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m base_nf_df\u001b[38;5;241m.\u001b[39mmerge(export_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariant SKU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVariant Compare At Price\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[0;32m     19\u001b[0m                              how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct variant SKU\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariant SKU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Step 4: Create 'Returned' and 'Offtake' columns\u001b[39;00m\n\u001b[0;32m     22\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReturned\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNet items sold\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNO\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['variant SKU'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\Export_2025-04-21_131843 (5).xlsx\")\n",
    "base_nf_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\base_nf.xlsx\")\n",
    "\n",
    "# Step 3: Merge MRP into base sales file using 'Product variant SKU' and 'variant SKU'\n",
    "merged_df = base_nf_df.merge(export_df[['variant SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='Product variant SKU', right_on='variant SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbulter', 'Truelove']\n",
    "vendor_df = merged_df[merged_df['Vendor'].isin(vendors)]\n",
    "\n",
    "# Step 6: Filter data for this week (Monday to Sunday)\n",
    "# Get the current date and calculate the start of the week (Monday)\n",
    "today = pd.Timestamp.today()\n",
    "start_of_week = today - pd.Timedelta(days=today.weekday())  # Start of the week (Monday)\n",
    "end_of_week = start_of_week + pd.Timedelta(days=6)  # End of the week (Sunday)\n",
    "\n",
    "# Filter data for this week based on 'Order Date'\n",
    "vendor_df['Order Date'] = pd.to_datetime(vendor_df['Order Date'])  # Ensure it's a datetime column\n",
    "this_week_sales = vendor_df[(vendor_df['Order Date'] >= start_of_week) & (vendor_df['Order Date'] <= end_of_week)]\n",
    "\n",
    "# Step 7: Group data for Dashboard (for this week)\n",
    "dashboard_df = this_week_sales.groupby(['Product title at time of sale', 'Product type']).agg({\n",
    "    'Offtake': 'sum',\n",
    "    'Net items sold': 'sum'  # Use sum for this week's sales\n",
    "}).reset_index()\n",
    "\n",
    "# Rename 'Net items sold' to 'Daily Sale Units' for clarity\n",
    "dashboard_df.rename(columns={'Net items sold': 'Daily Sale Units'}, inplace=True)\n",
    "\n",
    "# Step 8: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Pupscribe weekly sales dashboard').worksheet('Dashboard')\n",
    "dashboard_sheet.clear()  # Clear previous data\n",
    "\n",
    "# Update the sheet with the new data\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(\"This week's sales dashboard updated successfully 🚀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d2d11ce3-3d27-48c1-af7c-34ef524506ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This week's sales dashboard updated successfully 🚀\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\Export_2025-04-21_131843 (5).xlsx\")\n",
    "base_nf_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\base_nf.xlsx\")\n",
    "\n",
    "# Step 3: Merge MRP into base sales file using 'Product variant SKU' and 'Variant SKU'\n",
    "merged_df = base_nf_df.merge(export_df[['Variant SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='Product variant SKU', right_on='Variant SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbulter', 'Truelove']\n",
    "vendor_df = merged_df[merged_df['Product vendor'].isin(vendors)]\n",
    "\n",
    "# Step 6: Filter data for this week (Monday to Sunday)\n",
    "today = pd.Timestamp.today()\n",
    "start_of_week = today - pd.Timedelta(days=today.weekday())  # Start of the week (Monday)\n",
    "end_of_week = start_of_week + pd.Timedelta(days=6)  # End of the week (Sunday)\n",
    "\n",
    "# Filter the data to include only this week's sales\n",
    "vendor_df['Day'] = pd.to_datetime(vendor_df['Day'])  # Ensure 'Day' is a datetime column\n",
    "this_week_sales = vendor_df[(vendor_df['Day'] >= start_of_week) & (vendor_df['Day'] <= end_of_week)]\n",
    "\n",
    "# Step 7: Group data for Dashboard (for this week)\n",
    "dashboard_df = this_week_sales.groupby(['Product title at time of sale', 'Product type']).agg({\n",
    "    'Offtake': 'sum',  # Sum of total sales\n",
    "    'Net items sold': 'sum'  # Total sales units for this week\n",
    "}).reset_index()\n",
    "\n",
    "# Rename 'Net items sold' to 'Daily Sale Units'\n",
    "dashboard_df.rename(columns={'Net items sold': 'Daily Sale Units'}, inplace=True)\n",
    "\n",
    "# Step 8: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Pupscribe weekly  sales dashboard').worksheet('Dashboard')  # Adjust the name as needed\n",
    "dashboard_sheet.clear()  # Clear any previous data\n",
    "\n",
    "# Update the sheet with the new dashboard data\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(\"This week's sales dashboard updated successfully 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0174acc-4b2f-4a05-9b0c-a152d29429ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Supertails\\AppData\\Local\\Temp\\ipykernel_10396\\1566176330.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vendor_df['Day'] = pd.to_datetime(vendor_df['Day'])  # Make sure 'Day' is datetime\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales report for 14-Apr-2025 to 20-Apr-2025 updated successfully 🚀\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\Export_2025-04-21_131843 (5).xlsx\")\n",
    "base_nf_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\base_nf.xlsx\")\n",
    "\n",
    "# Step 3: Merge MRP into base sales file using 'Product variant SKU' and 'Variant SKU'\n",
    "merged_df = base_nf_df.merge(export_df[['Variant SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='Product variant SKU', right_on='Variant SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbutler', 'True Love']\n",
    "vendor_df = merged_df[merged_df['Product vendor'].isin(vendors)]\n",
    "\n",
    "# Step 6: Filter data between 14-04-2025 and 20-04-2025\n",
    "vendor_df['Day'] = pd.to_datetime(vendor_df['Day'])  # Make sure 'Day' is datetime\n",
    "\n",
    "start_date = pd.to_datetime('2025-04-14')\n",
    "end_date = pd.to_datetime('2025-04-20')\n",
    "\n",
    "# Filter the data for the given date range\n",
    "date_filtered_sales = vendor_df[(vendor_df['Day'] >= start_date) & (vendor_df['Day'] <= end_date)]\n",
    "\n",
    "# Step 7: Group data for Dashboard\n",
    "dashboard_df = date_filtered_sales.groupby(['Product title at time of sale', 'Product type']).agg({\n",
    "    'Offtake': 'sum',\n",
    "    'Net items sold': 'sum'  # Sum units sold\n",
    "}).reset_index()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Net items sold': 'Total Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 8: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Pupscribe weekly  sales dashboard').worksheet('Dashboard')  # Adjust the name if needed\n",
    "dashboard_sheet.clear()\n",
    "\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(\"Sales report for 14-Apr-2025 to 20-Apr-2025 updated successfully 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "474600e0-54d6-4324-be3b-8d5f3d424d1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Timestamp is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m dashboard_sheet \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPupscribe weekly  sales dashboard\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mworksheet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDashboard\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Adjust the name if needed\u001b[39;00m\n\u001b[0;32m     48\u001b[0m dashboard_sheet\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m---> 50\u001b[0m dashboard_sheet\u001b[38;5;241m.\u001b[39mupdate([dashboard_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()] \u001b[38;5;241m+\u001b[39m dashboard_df\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSales report for 14-Apr-2025 to 20-Apr-2025 updated successfully 🚀\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gspread\\worksheet.py:1242\u001b[0m, in \u001b[0;36mWorksheet.update\u001b[1;34m(self, values, range_name, raw, major_dimension, value_input_option, include_values_in_response, response_value_render_option, response_date_time_render_option)\u001b[0m\n\u001b[0;32m   1231\u001b[0m     value_input_option \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1232\u001b[0m         ValueInputOption\u001b[38;5;241m.\u001b[39mraw \u001b[38;5;28;01mif\u001b[39;00m raw \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ValueInputOption\u001b[38;5;241m.\u001b[39muser_entered\n\u001b[0;32m   1233\u001b[0m     )\n\u001b[0;32m   1235\u001b[0m params: ParamsType \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1236\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalueInputOption\u001b[39m\u001b[38;5;124m\"\u001b[39m: value_input_option,\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincludeValuesInResponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_values_in_response,\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponseValueRenderOption\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_value_render_option,\n\u001b[0;32m   1239\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponseDateTimeRenderOption\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_date_time_render_option,\n\u001b[0;32m   1240\u001b[0m }\n\u001b[1;32m-> 1242\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mvalues_update(\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspreadsheet_id,\n\u001b[0;32m   1244\u001b[0m     full_range_name,\n\u001b[0;32m   1245\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m   1246\u001b[0m     body\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m: values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmajorDimension\u001b[39m\u001b[38;5;124m\"\u001b[39m: major_dimension},\n\u001b[0;32m   1247\u001b[0m )\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gspread\\http_client.py:173\u001b[0m, in \u001b[0;36mHTTPClient.values_update\u001b[1;34m(self, id, range, params, body)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Lower-level method that directly calls `PUT spreadsheets/<ID>/values/<range> <https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets.values/update>`_.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m:param str range: The `A1 notation <https://developers.google.com/sheets/api/guides/concepts#a1_notation>`_ of the values to update.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m.. versionadded:: 3.0\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    172\u001b[0m url \u001b[38;5;241m=\u001b[39m SPREADSHEET_VALUES_URL \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mid\u001b[39m, quote(\u001b[38;5;28mrange\u001b[39m))\n\u001b[1;32m--> 173\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mput\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, json\u001b[38;5;241m=\u001b[39mbody)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gspread\\http_client.py:114\u001b[0m, in \u001b[0;36mHTTPClient.request\u001b[1;34m(self, method, endpoint, params, data, json, files, headers)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    106\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m     headers: Optional[MutableMapping[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    113\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m--> 114\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    115\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    116\u001b[0m         url\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m    117\u001b[0m         json\u001b[38;5;241m=\u001b[39mjson,\n\u001b[0;32m    118\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    119\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    120\u001b[0m         files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[0;32m    121\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    122\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\auth\\transport\\requests.py:537\u001b[0m, in \u001b[0;36mAuthorizedSession.request\u001b[1;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    534\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(AuthorizedSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    538\u001b[0m         method,\n\u001b[0;32m    539\u001b[0m         url,\n\u001b[0;32m    540\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    541\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest_headers,\n\u001b[0;32m    542\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    544\u001b[0m     )\n\u001b[0;32m    545\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[0;32m    547\u001b[0m \u001b[38;5;66;03m# If the response indicated that the credentials needed to be\u001b[39;00m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# refreshed, then refresh the credentials and re-attempt the\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;66;03m# request.\u001b[39;00m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# A stored token may expire between the time it is retrieved and\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# the time the request is made, so we may need to try twice.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[0;32m    563\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(\n\u001b[0;32m    564\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    565\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(req)\n\u001b[0;32m    577\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    579\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    580\u001b[0m     prep\u001b[38;5;241m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    581\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    481\u001b[0m     auth \u001b[38;5;241m=\u001b[39m get_netrc_auth(request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    483\u001b[0m p \u001b[38;5;241m=\u001b[39m PreparedRequest()\n\u001b[1;32m--> 484\u001b[0m p\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[0;32m    485\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    486\u001b[0m     url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m    487\u001b[0m     files\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mfiles,\n\u001b[0;32m    488\u001b[0m     data\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mdata,\n\u001b[0;32m    489\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mjson,\n\u001b[0;32m    490\u001b[0m     headers\u001b[38;5;241m=\u001b[39mmerge_setting(\n\u001b[0;32m    491\u001b[0m         request\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, dict_class\u001b[38;5;241m=\u001b[39mCaseInsensitiveDict\n\u001b[0;32m    492\u001b[0m     ),\n\u001b[0;32m    493\u001b[0m     params\u001b[38;5;241m=\u001b[39mmerge_setting(request\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams),\n\u001b[0;32m    494\u001b[0m     auth\u001b[38;5;241m=\u001b[39mmerge_setting(auth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth),\n\u001b[0;32m    495\u001b[0m     cookies\u001b[38;5;241m=\u001b[39mmerged_cookies,\n\u001b[0;32m    496\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mmerge_hooks(request\u001b[38;5;241m.\u001b[39mhooks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks),\n\u001b[0;32m    497\u001b[0m )\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:370\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_headers(headers)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_cookies(cookies)\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_body(data, files, json)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_auth(auth, url)\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Note that prepare_auth must be last to enable authentication schemes\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# such as OAuth to work on a fully prepared request.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# This MUST go after prepare_auth. Authenticators could add a hook\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:510\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_body\u001b[1;34m(self, data, files, json)\u001b[0m\n\u001b[0;32m    507\u001b[0m content_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 510\u001b[0m     body \u001b[38;5;241m=\u001b[39m complexjson\u001b[38;5;241m.\u001b[39mdumps(json, allow_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidJSONError(ve, request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    235\u001b[0m     skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[0;32m    236\u001b[0m     check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[0;32m    237\u001b[0m     separators\u001b[38;5;241m=\u001b[39mseparators, default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys,\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39mencode(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\encoder.py:200\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\encoder.py:258\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    255\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _iterencode(o, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type Timestamp is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\Export_2025-04-21_131843 (5).xlsx\")\n",
    "base_nf_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\base_nf.xlsx\")\n",
    "\n",
    "# Step 3: Merge MRP into base sales file using 'Product variant SKU' and 'Variant SKU'\n",
    "merged_df = base_nf_df.merge(export_df[['Variant SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='Product variant SKU', right_on='Variant SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbutler', 'True Love']\n",
    "vendor_df = merged_df[merged_df['Product vendor'].isin(vendors)].copy()\n",
    "\n",
    "# Step 6: Filter data between 14-04-2025 and 20-04-2025\n",
    "vendor_df['Day'] = pd.to_datetime(vendor_df['Day'])  # Make sure 'Day' is datetime\n",
    "\n",
    "start_date = pd.to_datetime('2025-04-14')\n",
    "end_date = pd.to_datetime('2025-04-20')\n",
    "\n",
    "# Filter the data for the given date range\n",
    "date_filtered_sales = vendor_df[(vendor_df['Day'] >= start_date) & (vendor_df['Day'] <= end_date)]\n",
    "\n",
    "# Step 7: Prepare Dashboard\n",
    "dashboard_df = date_filtered_sales[['Day', 'Product title at time of sale', 'Product type', 'Offtake', 'Net items sold']].copy()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Day': 'Order Date',\n",
    "    'Net items sold': 'Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 8: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Pupscribe weekly  sales dashboard').worksheet('Dashboard')  # Adjust the name if needed\n",
    "dashboard_sheet.clear()\n",
    "\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(\"Sales report for 14-Apr-2025 to 20-Apr-2025 updated successfully 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8e5d2395-c6fb-468e-86d2-3b6a76e7d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Prepare Dashboard\n",
    "dashboard_df = date_filtered_sales[['Day', 'Product title at time of sale', 'Product type', 'Offtake', 'Net items sold']].copy()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Day': 'Order Date',\n",
    "    'Net items sold': 'Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# ⭐ Fix: Convert 'Order Date' to string\n",
    "dashboard_df['Order Date'] = dashboard_df['Order Date'].dt.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c77be1ae-c162-4ee8-a475-5ef96195b2d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Timestamp is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m dashboard_sheet \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPupscribe weekly  sales dashboard\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mworksheet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDashboard\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Adjust the name if needed\u001b[39;00m\n\u001b[0;32m     48\u001b[0m dashboard_sheet\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m---> 50\u001b[0m dashboard_sheet\u001b[38;5;241m.\u001b[39mupdate([dashboard_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()] \u001b[38;5;241m+\u001b[39m dashboard_df\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSales report for 14-Apr-2025 to 20-Apr-2025 updated successfully 🚀\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gspread\\worksheet.py:1242\u001b[0m, in \u001b[0;36mWorksheet.update\u001b[1;34m(self, values, range_name, raw, major_dimension, value_input_option, include_values_in_response, response_value_render_option, response_date_time_render_option)\u001b[0m\n\u001b[0;32m   1231\u001b[0m     value_input_option \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1232\u001b[0m         ValueInputOption\u001b[38;5;241m.\u001b[39mraw \u001b[38;5;28;01mif\u001b[39;00m raw \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ValueInputOption\u001b[38;5;241m.\u001b[39muser_entered\n\u001b[0;32m   1233\u001b[0m     )\n\u001b[0;32m   1235\u001b[0m params: ParamsType \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1236\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalueInputOption\u001b[39m\u001b[38;5;124m\"\u001b[39m: value_input_option,\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincludeValuesInResponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_values_in_response,\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponseValueRenderOption\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_value_render_option,\n\u001b[0;32m   1239\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponseDateTimeRenderOption\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_date_time_render_option,\n\u001b[0;32m   1240\u001b[0m }\n\u001b[1;32m-> 1242\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mvalues_update(\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspreadsheet_id,\n\u001b[0;32m   1244\u001b[0m     full_range_name,\n\u001b[0;32m   1245\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m   1246\u001b[0m     body\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m: values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmajorDimension\u001b[39m\u001b[38;5;124m\"\u001b[39m: major_dimension},\n\u001b[0;32m   1247\u001b[0m )\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gspread\\http_client.py:173\u001b[0m, in \u001b[0;36mHTTPClient.values_update\u001b[1;34m(self, id, range, params, body)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Lower-level method that directly calls `PUT spreadsheets/<ID>/values/<range> <https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets.values/update>`_.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m:param str range: The `A1 notation <https://developers.google.com/sheets/api/guides/concepts#a1_notation>`_ of the values to update.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m.. versionadded:: 3.0\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    172\u001b[0m url \u001b[38;5;241m=\u001b[39m SPREADSHEET_VALUES_URL \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mid\u001b[39m, quote(\u001b[38;5;28mrange\u001b[39m))\n\u001b[1;32m--> 173\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mput\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, json\u001b[38;5;241m=\u001b[39mbody)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gspread\\http_client.py:114\u001b[0m, in \u001b[0;36mHTTPClient.request\u001b[1;34m(self, method, endpoint, params, data, json, files, headers)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    106\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m     headers: Optional[MutableMapping[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    113\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m--> 114\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    115\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    116\u001b[0m         url\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m    117\u001b[0m         json\u001b[38;5;241m=\u001b[39mjson,\n\u001b[0;32m    118\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    119\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    120\u001b[0m         files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[0;32m    121\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    122\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\auth\\transport\\requests.py:537\u001b[0m, in \u001b[0;36mAuthorizedSession.request\u001b[1;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    534\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(AuthorizedSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    538\u001b[0m         method,\n\u001b[0;32m    539\u001b[0m         url,\n\u001b[0;32m    540\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    541\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest_headers,\n\u001b[0;32m    542\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    544\u001b[0m     )\n\u001b[0;32m    545\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[0;32m    547\u001b[0m \u001b[38;5;66;03m# If the response indicated that the credentials needed to be\u001b[39;00m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# refreshed, then refresh the credentials and re-attempt the\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;66;03m# request.\u001b[39;00m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# A stored token may expire between the time it is retrieved and\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# the time the request is made, so we may need to try twice.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[0;32m    563\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(\n\u001b[0;32m    564\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    565\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(req)\n\u001b[0;32m    577\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    579\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    580\u001b[0m     prep\u001b[38;5;241m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    581\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    481\u001b[0m     auth \u001b[38;5;241m=\u001b[39m get_netrc_auth(request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    483\u001b[0m p \u001b[38;5;241m=\u001b[39m PreparedRequest()\n\u001b[1;32m--> 484\u001b[0m p\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[0;32m    485\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    486\u001b[0m     url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m    487\u001b[0m     files\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mfiles,\n\u001b[0;32m    488\u001b[0m     data\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mdata,\n\u001b[0;32m    489\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mjson,\n\u001b[0;32m    490\u001b[0m     headers\u001b[38;5;241m=\u001b[39mmerge_setting(\n\u001b[0;32m    491\u001b[0m         request\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, dict_class\u001b[38;5;241m=\u001b[39mCaseInsensitiveDict\n\u001b[0;32m    492\u001b[0m     ),\n\u001b[0;32m    493\u001b[0m     params\u001b[38;5;241m=\u001b[39mmerge_setting(request\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams),\n\u001b[0;32m    494\u001b[0m     auth\u001b[38;5;241m=\u001b[39mmerge_setting(auth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth),\n\u001b[0;32m    495\u001b[0m     cookies\u001b[38;5;241m=\u001b[39mmerged_cookies,\n\u001b[0;32m    496\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mmerge_hooks(request\u001b[38;5;241m.\u001b[39mhooks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks),\n\u001b[0;32m    497\u001b[0m )\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:370\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_headers(headers)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_cookies(cookies)\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_body(data, files, json)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_auth(auth, url)\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Note that prepare_auth must be last to enable authentication schemes\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# such as OAuth to work on a fully prepared request.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# This MUST go after prepare_auth. Authenticators could add a hook\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:510\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_body\u001b[1;34m(self, data, files, json)\u001b[0m\n\u001b[0;32m    507\u001b[0m content_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 510\u001b[0m     body \u001b[38;5;241m=\u001b[39m complexjson\u001b[38;5;241m.\u001b[39mdumps(json, allow_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidJSONError(ve, request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    235\u001b[0m     skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[0;32m    236\u001b[0m     check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[0;32m    237\u001b[0m     separators\u001b[38;5;241m=\u001b[39mseparators, default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys,\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39mencode(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\encoder.py:200\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\encoder.py:258\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    255\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _iterencode(o, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type Timestamp is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\Export_2025-04-21_131843 (5).xlsx\")\n",
    "base_nf_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\base_nf.xlsx\")\n",
    "\n",
    "# Step 3: Merge MRP into base sales file using 'Product variant SKU' and 'Variant SKU'\n",
    "merged_df = base_nf_df.merge(export_df[['Variant SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='Product variant SKU', right_on='Variant SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbutler', 'True Love']\n",
    "vendor_df = merged_df[merged_df['Product vendor'].isin(vendors)].copy()\n",
    "\n",
    "# Step 6: Filter data between 14-04-2025 and 20-04-2025\n",
    "vendor_df['Day'] = pd.to_datetime(vendor_df['Day'])  # Make sure 'Day' is datetime\n",
    "\n",
    "start_date = pd.to_datetime('2025-04-14')\n",
    "end_date = pd.to_datetime('2025-04-20')\n",
    "\n",
    "# Filter the data for the given date range\n",
    "date_filtered_sales = vendor_df[(vendor_df['Day'] >= start_date) & (vendor_df['Day'] <= end_date)]\n",
    "\n",
    "# Step 7: Prepare Dashboard\n",
    "dashboard_df = date_filtered_sales[['Day', 'Product title at time of sale', 'Product type', 'Offtake', 'Net items sold']].copy()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Day': 'Order Date',\n",
    "    'Net items sold': 'Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 8: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Pupscribe weekly  sales dashboard').worksheet('Dashboard')  # Adjust the name if needed\n",
    "dashboard_sheet.clear()\n",
    "\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(\"Sales report for 14-Apr-2025 to 20-Apr-2025 updated successfully 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "368b9255-6754-4a49-9de2-08b2dc1b7345",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_df = dashboard_df.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6fae91bf-2b17-4ee9-84ec-6e7b207a73dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1Uuox0mF9_EFO2CHml4Xly45G-Q57Yy6P9Kwfq8hBpR0',\n",
       " 'updatedRange': 'Dashboard!A1:E1784',\n",
       " 'updatedRows': 1784,\n",
       " 'updatedColumns': 5,\n",
       " 'updatedCells': 8920}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fa96e0b2-32a6-4ef7-8c09-36ec3b69511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_df = dashboard_df.sort_values(by='Offtake', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9861bd28-cd0c-447e-9319-3ac4ce616106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\Export_2025-04-21_131843 (5).xlsx\")\n",
    "base_nf_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\base_nf.xlsx\")\n",
    "\n",
    "# Step 3: Merge MRP into base sales file using 'Product variant SKU' and 'Variant SKU'\n",
    "merged_df = base_nf_df.merge(export_df[['Variant SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='Product variant SKU', right_on='Variant SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbutler', 'True Love']\n",
    "vendor_df = merged_df[merged_df['Product vendor'].isin(vendors)].copy()\n",
    "\n",
    "# Step 6: Filter data between 14-04-2025 and 20-04-2025\n",
    "vendor_df['Day'] = pd.to_datetime(vendor_df['Day'])  # Make sure 'Day' is datetime\n",
    "\n",
    "start_date = pd.to_datetime('2025-04-14')\n",
    "end_date = pd.to_datetime('2025-04-20')\n",
    "\n",
    "# Filter the data for the given date range\n",
    "date_filtered_sales = vendor_df[(vendor_df['Day'] >= start_date) & (vendor_df['Day'] <= end_date)]\n",
    "\n",
    "# Step 7: Prepare Dashboard\n",
    "dashboard_df = date_filtered_sales[['Day', 'Product title at time of sale', 'Product type', 'Offtake', 'Net items sold']].copy()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Day': 'Order Date',\n",
    "    'Net items sold': 'Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 8: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Pupscribe weekly  sales dashboard').worksheet('Dashboard')  # Adjust the name if needed\n",
    "dashboard_sheet.clear()\n",
    "\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(\"Sales report for 14-Apr-2025 to 20-Apr-2025 updated successfully 🚀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acefa28-75f4-402d-b1ef-d37b989f6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\Export_2025-04-21_131843 (5).xlsx\")\n",
    "base_nf_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\base_nf.xlsx\")\n",
    "\n",
    "# Step 3: Merge MRP into base sales file using 'Product variant SKU' and 'Variant SKU'\n",
    "merged_df = base_nf_df.merge(export_df[['Variant SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='Product variant SKU', right_on='Variant SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbutler', 'True Love']\n",
    "vendor_df = merged_df[merged_df['Product vendor'].isin(vendors)].copy()\n",
    "\n",
    "# Step 6: Filter data between 14-04-2025 and 20-04-2025\n",
    "vendor_df['Day'] = pd.to_datetime(vendor_df['Day'])  # Make sure 'Day' is datetime\n",
    "\n",
    "start_date = pd.to_datetime('2025-04-14')\n",
    "end_date = pd.to_datetime('2025-04-20')\n",
    "\n",
    "# Filter the data for the given date range\n",
    "date_filtered_sales = vendor_df[(vendor_df['Day'] >= start_date) & (vendor_df['Day'] <= end_date)]\n",
    "\n",
    "# Step 7: Prepare Dashboard\n",
    "dashboard_df = date_filtered_sales[['Day', 'Product title at time of sale', 'Product type', 'Offtake', 'Net items sold']].copy()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Day': 'Order Date',\n",
    "    'Net items sold': 'Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 8: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Pupscribe weekly  sales dashboard').worksheet('Dashboard')  # Adjust the name if needed\n",
    "dashboard_sheet.clear()\n",
    "\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(\"Sales report for 14-Apr-2025 to 20-Apr-2025 updated successfully 🚀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2675afd4-40ee-4832-86aa-8b1b71cfe5fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Timestamp is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m dashboard_sheet \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPupscribe weekly  sales dashboard\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mworksheet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDashboard\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Adjust the name if needed\u001b[39;00m\n\u001b[0;32m     51\u001b[0m dashboard_sheet\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m---> 53\u001b[0m dashboard_sheet\u001b[38;5;241m.\u001b[39mupdate([dashboard_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()] \u001b[38;5;241m+\u001b[39m dashboard_df\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     54\u001b[0m dashboard_df \u001b[38;5;241m=\u001b[39m dashboard_df\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSales report for 14-Apr-2025 to 20-Apr-2025 updated successfully 🚀\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gspread\\worksheet.py:1242\u001b[0m, in \u001b[0;36mWorksheet.update\u001b[1;34m(self, values, range_name, raw, major_dimension, value_input_option, include_values_in_response, response_value_render_option, response_date_time_render_option)\u001b[0m\n\u001b[0;32m   1231\u001b[0m     value_input_option \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1232\u001b[0m         ValueInputOption\u001b[38;5;241m.\u001b[39mraw \u001b[38;5;28;01mif\u001b[39;00m raw \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ValueInputOption\u001b[38;5;241m.\u001b[39muser_entered\n\u001b[0;32m   1233\u001b[0m     )\n\u001b[0;32m   1235\u001b[0m params: ParamsType \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1236\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalueInputOption\u001b[39m\u001b[38;5;124m\"\u001b[39m: value_input_option,\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincludeValuesInResponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_values_in_response,\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponseValueRenderOption\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_value_render_option,\n\u001b[0;32m   1239\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponseDateTimeRenderOption\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_date_time_render_option,\n\u001b[0;32m   1240\u001b[0m }\n\u001b[1;32m-> 1242\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mvalues_update(\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspreadsheet_id,\n\u001b[0;32m   1244\u001b[0m     full_range_name,\n\u001b[0;32m   1245\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m   1246\u001b[0m     body\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m: values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmajorDimension\u001b[39m\u001b[38;5;124m\"\u001b[39m: major_dimension},\n\u001b[0;32m   1247\u001b[0m )\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gspread\\http_client.py:173\u001b[0m, in \u001b[0;36mHTTPClient.values_update\u001b[1;34m(self, id, range, params, body)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Lower-level method that directly calls `PUT spreadsheets/<ID>/values/<range> <https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets.values/update>`_.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m:param str range: The `A1 notation <https://developers.google.com/sheets/api/guides/concepts#a1_notation>`_ of the values to update.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m.. versionadded:: 3.0\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    172\u001b[0m url \u001b[38;5;241m=\u001b[39m SPREADSHEET_VALUES_URL \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mid\u001b[39m, quote(\u001b[38;5;28mrange\u001b[39m))\n\u001b[1;32m--> 173\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mput\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, json\u001b[38;5;241m=\u001b[39mbody)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gspread\\http_client.py:114\u001b[0m, in \u001b[0;36mHTTPClient.request\u001b[1;34m(self, method, endpoint, params, data, json, files, headers)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    106\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m     headers: Optional[MutableMapping[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    113\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m--> 114\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    115\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    116\u001b[0m         url\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m    117\u001b[0m         json\u001b[38;5;241m=\u001b[39mjson,\n\u001b[0;32m    118\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    119\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    120\u001b[0m         files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[0;32m    121\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    122\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\auth\\transport\\requests.py:537\u001b[0m, in \u001b[0;36mAuthorizedSession.request\u001b[1;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    534\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(AuthorizedSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    538\u001b[0m         method,\n\u001b[0;32m    539\u001b[0m         url,\n\u001b[0;32m    540\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    541\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest_headers,\n\u001b[0;32m    542\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    544\u001b[0m     )\n\u001b[0;32m    545\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[0;32m    547\u001b[0m \u001b[38;5;66;03m# If the response indicated that the credentials needed to be\u001b[39;00m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# refreshed, then refresh the credentials and re-attempt the\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;66;03m# request.\u001b[39;00m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# A stored token may expire between the time it is retrieved and\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# the time the request is made, so we may need to try twice.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[0;32m    563\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(\n\u001b[0;32m    564\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    565\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(req)\n\u001b[0;32m    577\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    579\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    580\u001b[0m     prep\u001b[38;5;241m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    581\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    481\u001b[0m     auth \u001b[38;5;241m=\u001b[39m get_netrc_auth(request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    483\u001b[0m p \u001b[38;5;241m=\u001b[39m PreparedRequest()\n\u001b[1;32m--> 484\u001b[0m p\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[0;32m    485\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    486\u001b[0m     url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m    487\u001b[0m     files\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mfiles,\n\u001b[0;32m    488\u001b[0m     data\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mdata,\n\u001b[0;32m    489\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mjson,\n\u001b[0;32m    490\u001b[0m     headers\u001b[38;5;241m=\u001b[39mmerge_setting(\n\u001b[0;32m    491\u001b[0m         request\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, dict_class\u001b[38;5;241m=\u001b[39mCaseInsensitiveDict\n\u001b[0;32m    492\u001b[0m     ),\n\u001b[0;32m    493\u001b[0m     params\u001b[38;5;241m=\u001b[39mmerge_setting(request\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams),\n\u001b[0;32m    494\u001b[0m     auth\u001b[38;5;241m=\u001b[39mmerge_setting(auth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth),\n\u001b[0;32m    495\u001b[0m     cookies\u001b[38;5;241m=\u001b[39mmerged_cookies,\n\u001b[0;32m    496\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mmerge_hooks(request\u001b[38;5;241m.\u001b[39mhooks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks),\n\u001b[0;32m    497\u001b[0m )\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:370\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_headers(headers)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_cookies(cookies)\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_body(data, files, json)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_auth(auth, url)\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Note that prepare_auth must be last to enable authentication schemes\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# such as OAuth to work on a fully prepared request.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# This MUST go after prepare_auth. Authenticators could add a hook\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:510\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_body\u001b[1;34m(self, data, files, json)\u001b[0m\n\u001b[0;32m    507\u001b[0m content_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 510\u001b[0m     body \u001b[38;5;241m=\u001b[39m complexjson\u001b[38;5;241m.\u001b[39mdumps(json, allow_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidJSONError(ve, request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    235\u001b[0m     skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[0;32m    236\u001b[0m     check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[0;32m    237\u001b[0m     separators\u001b[38;5;241m=\u001b[39mseparators, default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys,\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39mencode(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\encoder.py:200\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\encoder.py:258\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    255\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _iterencode(o, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type Timestamp is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\Export_2025-04-21_131843 (5).xlsx\")\n",
    "base_nf_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\base_nf.xlsx\")\n",
    "\n",
    "# Step 3: Merge MRP into base sales file using 'Product variant SKU' and 'Variant SKU'\n",
    "merged_df = base_nf_df.merge(export_df[['Variant SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='Product variant SKU', right_on='Variant SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbutler', 'True Love']\n",
    "vendor_df = merged_df[merged_df['Product vendor'].isin(vendors)].copy()\n",
    "\n",
    "# Step 6: Filter data between 14-04-2025 and 20-04-2025\n",
    "vendor_df['Day'] = pd.to_datetime(vendor_df['Day'])  # Make sure 'Day' is datetime\n",
    "\n",
    "start_date = pd.to_datetime('2025-04-14')\n",
    "end_date = pd.to_datetime('2025-04-20')\n",
    "\n",
    "# Filter the data for the given date range\n",
    "date_filtered_sales = vendor_df[(vendor_df['Day'] >= start_date) & (vendor_df['Day'] <= end_date)]\n",
    "\n",
    "# Step 7: Prepare Dashboard\n",
    "dashboard_df = date_filtered_sales[['Day', 'Product title at time of sale', 'Product type', 'Offtake', 'Net items sold']].copy()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Day': 'Order Date',\n",
    "    'Net items sold': 'Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 8: Sort by Offtake in increasing order\n",
    "dashboard_df = dashboard_df.sort_values(by='Offtake', ascending=False)\n",
    "\n",
    "# Step 9: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Pupscribe weekly  sales dashboard').worksheet('Dashboard')  # Adjust the name if needed\n",
    "dashboard_sheet.clear()\n",
    "\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "dashboard_df = dashboard_df.astype(str)\n",
    "\n",
    "\n",
    "print(\"Sales report for 14-Apr-2025 to 20-Apr-2025 updated successfully 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "811b9662-0b29-4591-ae0c-613c1b3ac12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales report for 14-Apr-2025 to 20-Apr-2025 updated successfully 🚀\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\Export_2025-04-21_131843 (5).xlsx\")\n",
    "base_nf_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\base_nf.xlsx\")\n",
    "\n",
    "# Step 3: Merge MRP into base sales file using 'Product variant SKU' and 'Variant SKU'\n",
    "merged_df = base_nf_df.merge(export_df[['Variant SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='Product variant SKU', right_on='Variant SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbutler', 'True Love']\n",
    "vendor_df = merged_df[merged_df['Product vendor'].isin(vendors)].copy()\n",
    "\n",
    "# Step 6: Filter data between 14-04-2025 and 20-04-2025\n",
    "vendor_df['Day'] = pd.to_datetime(vendor_df['Day'])  # Ensure 'Day' is datetime\n",
    "\n",
    "start_date = pd.to_datetime('2025-04-14')\n",
    "end_date = pd.to_datetime('2025-04-20')\n",
    "\n",
    "# Filter the data for the given date range\n",
    "date_filtered_sales = vendor_df[(vendor_df['Day'] >= start_date) & (vendor_df['Day'] <= end_date)]\n",
    "\n",
    "# Step 7: Prepare Dashboard\n",
    "dashboard_df = date_filtered_sales[['Day', 'Product title at time of sale', 'Product type', 'Offtake', 'Net items sold']].copy()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Day': 'Order Date',\n",
    "    'Net items sold': 'Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 8: Sort by Offtake in descending order\n",
    "dashboard_df = dashboard_df.sort_values(by='Offtake', ascending=False)\n",
    "\n",
    "# Step 9: Convert any datetime columns to string format\n",
    "# Convert 'Order Date' column to string format to avoid JSON serialization issues\n",
    "dashboard_df['Order Date'] = dashboard_df['Order Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Step 10: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Pupscribe weekly  sales dashboard').worksheet('Dashboard')  # Adjust the name if needed\n",
    "dashboard_sheet.clear()\n",
    "\n",
    "# Update the sheet with the sorted data\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(\"Sales report for 14-Apr-2025 to 20-Apr-2025 updated successfully 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "03607a85-1f8e-43df-96d0-d7308bc6bbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report for 14-Apr-2025 to 20-Apr-2025\n",
      "Sales report for 14-Apr-2025 to 20-Apr-2025 updated successfully 🚀\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\Export_2025-04-21_131843 (5).xlsx\")\n",
    "base_nf_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\base_nf.xlsx\")\n",
    "\n",
    "# Step 3: Merge MRP into base sales file using 'Product variant SKU' and 'Variant SKU'\n",
    "merged_df = base_nf_df.merge(export_df[['Variant SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='Product variant SKU', right_on='Variant SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbutler', 'True Love']\n",
    "vendor_df = merged_df[merged_df['Product vendor'].isin(vendors)].copy()\n",
    "\n",
    "# Step 6: Calculate previous week's Monday to Sunday\n",
    "today = datetime.today()\n",
    "start_date = today - timedelta(days=today.weekday() + 7)   # Previous Monday\n",
    "end_date = start_date + timedelta(days=6)                  # Previous Sunday\n",
    "\n",
    "print(f\"Generating report for {start_date.strftime('%d-%b-%Y')} to {end_date.strftime('%d-%b-%Y')}\")\n",
    "\n",
    "# Step 7: Filter data for previous week\n",
    "vendor_df['Day'] = pd.to_datetime(vendor_df['Day'])  # Ensure 'Day' is datetime\n",
    "\n",
    "date_filtered_sales = vendor_df[(vendor_df['Day'] >= start_date) & (vendor_df['Day'] <= end_date)]\n",
    "\n",
    "# Step 8: Prepare Dashboard\n",
    "dashboard_df = date_filtered_sales[['Day', 'Product title at time of sale', 'Product type', 'Offtake', 'Net items sold']].copy()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Day': 'Order Date',\n",
    "    'Net items sold': 'Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 9: Sort by Offtake in descending order\n",
    "dashboard_df = dashboard_df.sort_values(by='Offtake', ascending=False)\n",
    "\n",
    "# Step 10: Convert any datetime columns to string format\n",
    "dashboard_df['Order Date'] = dashboard_df['Order Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Step 11: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Pupscribe weekly  sales dashboard').worksheet('Dashboard')  # Adjust the name if needed\n",
    "dashboard_sheet.clear()\n",
    "\n",
    "# Update the sheet with the sorted data\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(f\"Sales report for {start_date.strftime('%d-%b-%Y')} to {end_date.strftime('%d-%b-%Y')} updated successfully 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b2e30e97-b142-4234-acd9-c6825a0d5758",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (844969820.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[123], line 19\u001b[1;36m\u001b[0m\n\u001b[1;33m    les file using 'Product variant SKU' and 'Variant SKU'\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\Export_2025-04-21_131843 (5).xlsx\")\n",
    "base_nf_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\base_nf.xlsx\")\n",
    "\n",
    "# Step 3: Merge MRP into base sa\n",
    "les file using 'Product variant SKU' and 'Variant SKU'\n",
    "merged_df = base_nf_df.merge(export_df[['Variant SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='Product variant SKU', right_on='Variant SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbutler', 'True Love']\n",
    "vendor_df = merged_df[merged_df['Product vendor'].isin(vendors)].copy()\n",
    "\n",
    "# Step 6: Calculate previous week's Monday to Sunday properly\n",
    "today = datetime.today()\n",
    "\n",
    "# Get the last Monday\n",
    "last_monday = today - timedelta(days=today.weekday() + 7)\n",
    "last_sunday = last_monday + timedelta(days=6)\n",
    "\n",
    "start_date = last_monday.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "end_date = last_sunday.replace(hour=23, minute=59, second=59, microsecond=999999)\n",
    "\n",
    "print(f\"Generating report for {start_date.strftime('%d-%b-%Y')} to {end_date.strftime('%d-%b-%Y')}\")\n",
    "\n",
    "# Step 7: Filter data for previous week\n",
    "vendor_df['Day'] = pd.to_datetime(vendor_df['Day'])  # Ensure 'Day' is datetime\n",
    "\n",
    "date_filtered_sales = vendor_df[(vendor_df['Day'] >= start_date) & (vendor_df['Day'] <= end_date)]\n",
    "\n",
    "# Step 8: Prepare Dashboard\n",
    "dashboard_df = date_filtered_sales[['Day', 'Product title at time of sale', 'Product type', 'Offtake', 'Net items sold']].copy()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Day': 'Order Date',\n",
    "    'Net items sold': 'Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 9: Sort by Offtake in descending order\n",
    "dashboard_df = dashboard_df.sort_values(by='Offtake', ascending=False)\n",
    "\n",
    "# Step 10: Convert any datetime columns to string format\n",
    "dashboard_df['Order Date'] = dashboard_df['Order Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Step 11: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Pupscribe weekly  sales dashboard').worksheet('Dashboard')  # Adjust the name if needed\n",
    "dashboard_sheet.clear()\n",
    "\n",
    "# Update the sheet with the sorted data\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(f\"Sales report for {start_date.strftime('%d-%b-%Y')} to {end_date.strftime('%d-%b-%Y')} updated successfully 🚀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "466b7edb-8598-41a5-8e17-5e7c51483c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales report for 14-Apr-2025 to 20-Apr-2025 updated successfully 🚀\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\Export_2025-04-21_131843 (5).xlsx\")\n",
    "base_nf_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\base_nf.xlsx\")\n",
    "\n",
    "# Step 3: Merge MRP into base sales file using 'Product variant SKU' and 'Variant SKU'\n",
    "merged_df = base_nf_df.merge(export_df[['Variant SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='Product variant SKU', right_on='Variant SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbutler', 'True Love']\n",
    "vendor_df = merged_df[merged_df['Product vendor'].isin(vendors)].copy()\n",
    "\n",
    "# Step 6: Filter data between 14-04-2025 and 20-04-2025\n",
    "vendor_df['Day'] = pd.to_datetime(vendor_df['Day'])  # Ensure 'Day' is datetime\n",
    "\n",
    "start_date = pd.to_datetime('2025-04-14')\n",
    "end_date = pd.to_datetime('2025-04-20')\n",
    "\n",
    "# Filter the data for the given date range\n",
    "date_filtered_sales = vendor_df[(vendor_df['Day'] >= start_date) & (vendor_df['Day'] <= end_date)]\n",
    "\n",
    "# Step 7: Prepare Dashboard\n",
    "dashboard_df = date_filtered_sales[['Day', 'Product title at time of sale', 'Product type', 'Offtake', 'Net items sold']].copy()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Day': 'Order Date',\n",
    "    'Net items sold': 'Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 8: Sort by Offtake in descending order\n",
    "dashboard_df = dashboard_df.sort_values(by='Offtake', ascending=False)\n",
    "\n",
    "# Step 9: Convert any datetime columns to string format\n",
    "# Convert 'Order Date' column to string format to avoid JSON serialization issues\n",
    "dashboard_df['Order Date'] = dashboard_df['Order Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Step 10: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Pupscribe weekly  sales dashboard').worksheet('Dashboard')  # Adjust the name if needed\n",
    "dashboard_sheet.clear()\n",
    "\n",
    "# Update the sheet with the sorted data\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(\"Sales report for 14-Apr-2025 to 20-Apr-2025 updated successfully 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5ea066b7-e034-4d81-bffe-eed185da7d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales report updated successfully 🚀\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Step 1: Setup credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = 'service_account_key.json'\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Step 2: Load Excel files\n",
    "export_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\Export_2025-04-21_131843 (5).xlsx\")\n",
    "base_nf_df = pd.read_excel(r\"C:\\Users\\Supertails\\Downloads\\base_nf.xlsx\")\n",
    "\n",
    "# Step 3: Merge MRP into base sales file using 'Product variant SKU' and 'Variant SKU'\n",
    "merged_df = base_nf_df.merge(export_df[['Variant SKU', 'Variant Compare At Price']],\n",
    "                             how='left', left_on='Product variant SKU', right_on='Variant SKU')\n",
    "\n",
    "# Step 4: Create 'Returned' and 'Offtake' columns\n",
    "merged_df['Returned'] = merged_df['Net items sold'].apply(lambda x: 'YES' if x < 0 else 'NO')\n",
    "merged_df['Offtake'] = merged_df['Net items sold'] * merged_df['Variant Compare At Price']\n",
    "\n",
    "# Step 5: Filter vendors\n",
    "vendors = ['Barkbutler', 'True Love']\n",
    "vendor_df = merged_df[merged_df['Product vendor'].isin(vendors)].copy()\n",
    "\n",
    "# Step 6: Filter data between 14-04-2025 and 20-04-2025\n",
    "vendor_df['Day'] = pd.to_datetime(vendor_df['Day'])  # Ensure 'Day' is datetime\n",
    "\n",
    "start_date = pd.to_datetime('2025-04-14')\n",
    "end_date = pd.to_datetime('2025-04-20')\n",
    "\n",
    "# Filter the data for the given date range\n",
    "date_filtered_sales = vendor_df[(vendor_df['Day'] >= start_date) & (vendor_df['Day'] <= end_date)]\n",
    "\n",
    "# Step 7: Prepare Dashboard\n",
    "dashboard_df = date_filtered_sales[['Day', 'Product title at time of sale', 'Product type', 'Product variant SKU', 'Offtake', 'Net items sold']].copy()\n",
    "\n",
    "dashboard_df.rename(columns={\n",
    "    'Day': 'Order Date',\n",
    "    'Product variant SKU': 'Variant SKU',  # <-- Rename for clean header\n",
    "    'Net items sold': 'Sale Units',\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 8: Sort by Offtake in descending order\n",
    "dashboard_df = dashboard_df.sort_values(by='Offtake', ascending=False)\n",
    "\n",
    "# Step 9: Convert datetime columns to string\n",
    "dashboard_df['Order Date'] = dashboard_df['Order Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Step 10: Upload to Google Sheets\n",
    "dashboard_sheet = client.open('Pupscribe weekly  sales dashboard').worksheet('Dashboard')  # Adjust the name if needed\n",
    "dashboard_sheet.clear()\n",
    "\n",
    "dashboard_sheet.update([dashboard_df.columns.values.tolist()] + dashboard_df.values.tolist())\n",
    "\n",
    "print(\"Sales report updated successfully 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078803a8-9485-4656-b620-dba8fa1b6262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
